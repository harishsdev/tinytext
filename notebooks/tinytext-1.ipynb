{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6af309ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis repository will consist of:\\n\\n [text.data](#data) : Generic data loaders, abstractions, and iterators for text\\n [text.datasets](#datasets) : Pre-built loaders for common NLP datasets\\n(maybe) text.models : Model definitions and pre-trained models for popular NLP examples\\n(though the situation is not the same as vision, where people can download a pretrained ImageNet model and immediately\\nmake it useful for other tasks -- it might make more sense to leave NLP models in the torch/examples repo)\\n\\n# Data\\n\\nThe data module should provide the following:\\n\\n- Ability to describe declaratively how to load a custom NLP dataset that\\'s in a \"normal\" format:\\n\\n\\nExample 1:-\\n\\npos = data.Dataset(path=\\'data/pos/pos_wsj_train.tsv\\', format=\\'tsv\\',fields=[(\\'text\\', data.Field(time_series=True),\\n                                                                            \\n                                                                            (\\'labels\\', data.Field(time_series=True)])\\n\\nExample 2:-\\n\\nsentiment = data.Dataset(\\n    path=\\'data/sentiment/train.json\\', format=\\'json\\',\\n    fields=[{\\'sentence_tokenized\\': (\\'text\\', data.Field(time_series=True)),\\n            (\\'sentiment_gold\\': (\\'labels\\', data.Field(time_series=False))])\\n\\n\\nAbility to define a preprocessing pipeline:\\n\\nExample 3:-\\n\\nsrc = data.Field(time_series=True, preprocess=my_custom_tokenizer)\\ntrg = data.Field(time_series=True, preprocess=my_custom_tokenizer)\\nmt_train = data.ParallelCorpus(\\n    path=\\'data/mt/wmt16-ende.train\\', exts=(\\'.en\\', \\'.de\\'),\\n    fields=((\\'src\\', src), (\\'trg\\', trg))\\n\\nBatching, padding, and numericalizing (including building a vocabulary object):\\n\\nExample 4:-\\n\\n# continuing from above\\nmt_dev = data.ParallelCorpus(\\n    path=\\'data/mt/newstest2014\\', exts=(\\'.en\\', \\'.de\\'),\\n    fields=((\\'src\\', src), (\\'trg\\', trg))\\nsrc.build_vocab(mt_train.src, max_size=80000)\\ntrg.build_vocab(mt_train.trg, max_size=40000)\\n\\n# mt_dev shares the fields, so it shares their vocab objects\\n\\nExample 5:-\\n\\ntrain_iter = data.BucketIterator(\\n    batch_size=32, mt_train,\\n    sort_key=lambda x: data.interleave_keys(len(x.src), len(x.trg)))\\n# usage\\n>>>next(train_iter)\\n+<data.Batch(batch_size=32, src=[LongTensor (32, 25)], trg=[LongTensor (32, 28)])>\\n+```\\n+\\n\\n\\n# Datasets\\n+\\n+Some datasets it would be useful to have built in:\\n+\\n+- bAbI and successors from FAIR\\n+- SST and IMDb sentiment\\n+- SNLI\\n+- Penn Treebank (for language modeling and parsing)\\n+- WMT and/or IWSLT machine translation\\n+- SQuAD\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "This repository will consist of:\n",
    "\n",
    " [text.data](#data) : Generic data loaders, abstractions, and iterators for text\n",
    " [text.datasets](#datasets) : Pre-built loaders for common NLP datasets\n",
    "(maybe) text.models : Model definitions and pre-trained models for popular NLP examples\n",
    "(though the situation is not the same as vision, where people can download a pretrained ImageNet model and immediately\n",
    "make it useful for other tasks -- it might make more sense to leave NLP models in the torch/examples repo)\n",
    "\n",
    "# Data\n",
    "\n",
    "The data module should provide the following:\n",
    "\n",
    "- Ability to describe declaratively how to load a custom NLP dataset that's in a \"normal\" format:\n",
    "\n",
    "\n",
    "Example 1:-\n",
    "\n",
    "pos = data.Dataset(path='data/pos/pos_wsj_train.tsv', format='tsv',fields=[('text', data.Field(time_series=True),\n",
    "                                                                            \n",
    "                                                                            ('labels', data.Field(time_series=True)])\n",
    "\n",
    "Example 2:-\n",
    "\n",
    "sentiment = data.Dataset(\n",
    "    path='data/sentiment/train.json', format='json',\n",
    "    fields=[{'sentence_tokenized': ('text', data.Field(time_series=True)),\n",
    "            ('sentiment_gold': ('labels', data.Field(time_series=False))])\n",
    "\n",
    "\n",
    "Ability to define a preprocessing pipeline:\n",
    "\n",
    "Example 3:-\n",
    "\n",
    "src = data.Field(time_series=True, preprocess=my_custom_tokenizer)\n",
    "trg = data.Field(time_series=True, preprocess=my_custom_tokenizer)\n",
    "mt_train = data.ParallelCorpus(\n",
    "    path='data/mt/wmt16-ende.train', exts=('.en', '.de'),\n",
    "    fields=(('src', src), ('trg', trg))\n",
    "\n",
    "Batching, padding, and numericalizing (including building a vocabulary object):\n",
    "\n",
    "Example 4:-\n",
    "\n",
    "# continuing from above\n",
    "mt_dev = data.ParallelCorpus(\n",
    "    path='data/mt/newstest2014', exts=('.en', '.de'),\n",
    "    fields=(('src', src), ('trg', trg))\n",
    "src.build_vocab(mt_train.src, max_size=80000)\n",
    "trg.build_vocab(mt_train.trg, max_size=40000)\n",
    "\n",
    "# mt_dev shares the fields, so it shares their vocab objects\n",
    "\n",
    "Example 5:-\n",
    "\n",
    "train_iter = data.BucketIterator(\n",
    "    batch_size=32, mt_train,\n",
    "    sort_key=lambda x: data.interleave_keys(len(x.src), len(x.trg)))\n",
    "# usage\n",
    ">>>next(train_iter)\n",
    "+<data.Batch(batch_size=32, src=[LongTensor (32, 25)], trg=[LongTensor (32, 28)])>\n",
    "+```\n",
    "+\n",
    "\n",
    "\n",
    "# Datasets\n",
    "+\n",
    "+Some datasets it would be useful to have built in:\n",
    "+\n",
    "+- bAbI and successors from FAIR\n",
    "+- SST and IMDb sentiment\n",
    "+- SNLI\n",
    "+- Penn Treebank (for language modeling and parsing)\n",
    "+- WMT and/or IWSLT machine translation\n",
    "+- SQuAD\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "953033fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-3ca271ba3233>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-3ca271ba3233>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    +from text.torchtext import data\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#import torch\n",
    "+from text.torchtext import data\n",
    "+\n",
    "+\n",
    "+TEXT = data.Field(time_series=True)\n",
    "+LABELS = data.Field(time_series=True)\n",
    "+\n",
    "+train = data.Dataset(path='~/data/pos_wsj/pos_wsj.train',\n",
    "+                     format='tsv', fields=[('text', TEXT), ('labels', LABELS)])\n",
    "+\n",
    "+print(train.fields)\n",
    "+print(vars(train[0]))\n",
    "+\n",
    "+train_iter = data.BucketIterator(\n",
    "+    train, batch_size=3, key=lambda x: len(x.text), device=0)\n",
    "+\n",
    "+LABELS.build_vocab(train.labels)\n",
    "+TEXT.build_vocab(train.text)\n",
    "+\n",
    "+print(TEXT.vocab.freqs.most_common(10))\n",
    "+print(LABELS.vocab.itos)\n",
    "+\n",
    "+batch = next(iter(train_iter))\n",
    "+print(batch.text)\n",
    "+print(batch.labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "+++ b/test/vocab.py\n",
    "@@ -0,0 +1,9 @@\n",
    "+import torch\n",
    "+from text.torchtext import vocab\n",
    "+\n",
    "+from collections import Counter\n",
    "+c = Counter(['hello', 'world'])\n",
    "+v = vocab.Vocab(c, wv_path='/home/james.bradbury/chainer-research/util/'\n",
    "+                'glove.840B.300d')\n",
    "+print(v.itos)\n",
    "+print(v.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a1b75a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
